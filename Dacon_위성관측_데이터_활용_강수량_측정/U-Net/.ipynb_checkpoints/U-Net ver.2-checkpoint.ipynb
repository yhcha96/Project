{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L8a2GwqnPlZf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('drive/My Drive/Colab Notebooks/Project/AIFrenz_Season2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DQgvZgIPlWU"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3nrYREAQHd_"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7eAGM57PlTE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split , KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kQjGVDRIEOF"
   },
   "outputs": [],
   "source": [
    "np.random.seed(777)\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mltwli5EjyOi"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDcNxy68jyjZ"
   },
   "outputs": [],
   "source": [
    "WIDTH = 40\n",
    "HEIGHT = 40\n",
    "CHANNEL = 9\n",
    "IMAGE_SIZE = [WIDTH,HEIGHT,CHANNEL]\n",
    "numOutputClass  = 1\n",
    "\n",
    "file_path = 'data/dataset'\n",
    "output_path = 'ckpt/output'\n",
    "pred_path = 'ckpt/prediction'\n",
    "\n",
    "NUM_TRAIN = 76345\n",
    "NUM_TEST = 2416\n",
    "BATCH_SIZE = 512\n",
    "TOTAL_BATCH = int(NUM_TRAIN/BATCH_SIZE)\n",
    "EPOCHS = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzn0Zjqej6NB"
   },
   "outputs": [],
   "source": [
    "train_path = './data/dataset/'      ##### path 수정\n",
    "image_path = 'train_images.npy'     ##### path 수정\n",
    "data_path = 'train_data.npy'        ##### path 수정\n",
    "label_path = 'train_labels.npy'     ##### path 수정\n",
    "\n",
    "train_image = np.load(train_path+image_path)\n",
    "#train_data = np.load(train_path+data_path)\n",
    "train_label = np.load(train_path+label_path)\n",
    "#train_all = np.concatenate([train_image,train_label],axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6qYy2CAkhLq"
   },
   "outputs": [],
   "source": [
    "# [이미지 증식 코드]\n",
    "# img_datagen = ImageDataGenerator(rotation_range=20,\n",
    "#                                  width_shift_range=0.1,\n",
    "#                                  height_shift_range=0.1)\n",
    "# genTrain = img_datagen.flow(train, batch_size=train.shape[0], shuffle=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilsJ-xSQhJ-g"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_path = './data/dataset/'\n",
    "# image_path = 'train_imageDataGenerator.npy'\n",
    "\n",
    "# genData = np.load(train_path+image_path)\n",
    "\n",
    "# train_image = genData[:,:,:,:9]\n",
    "# train_label = genData[:,:,:,-1]\n",
    "# del genData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0kEj5qxoBdG"
   },
   "outputs": [],
   "source": [
    "# Load test_images\n",
    "path = './data/dataset/' \n",
    "test_image = np.load(path+'test_images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "De70LZGpH0RK"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_image, train_label, test_size=0.05, random_state=777)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "\n",
    "del train_image , train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2zs4LcFd57_"
   },
   "outputs": [],
   "source": [
    "# 0보다 작은 값이 포함된 데이터 삭제\n",
    "y_train_ = y_train.reshape(-1, y_train.shape[1]*y_train.shape[2])\n",
    "\n",
    "x_train = np.delete(x_train, np.where(y_train_ < 0)[0], axis=0)\n",
    "y_train = np.delete(y_train, np.where(y_train_ < 0)[0], axis=0)\n",
    "y_train = y_train.reshape(-1, x_train.shape[1], x_train.shape[2],1)\n",
    "y_test = y_test.reshape(-1, y_test.shape[1], y_test.shape[2],1)\n",
    "\n",
    "y_train_ = np.delete(y_train_, np.where(y_train_ < 0)[0], axis=0)\n",
    "\n",
    "# # 강수량이 50이상인 데이터만 선택\n",
    "# x_train = x_train[np.sum(y_train_ >= 0)]\n",
    "# y_train = y_train[np.sum(y_train_ >= 50)]\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ST5SYe89k6Db"
   },
   "outputs": [],
   "source": [
    "#train_label_fit = train_label.reshape(-1,1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9q9Xrjxj92X"
   },
   "outputs": [],
   "source": [
    "# def trainGenerator():\n",
    "#     for i in range(len(train_label)):\n",
    "#         Xdata = train_image[i,:,:,:]\n",
    "#         ydata = train_label[i,:,:,:]\n",
    "#         ydata = ydata.reshape([-1,40*40*1])\n",
    "\n",
    "#         yield (Xdata , ydata)\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_generator(trainGenerator,(tf.float32,tf.float32),(tf.TensorShape([40,40,9]),tf.TensorShape([None,40*40*1])))    ##### shape 수정\n",
    "\n",
    "# train_dataset = train_dataset.repeat(EPOCHS).batch(BATCH_SIZE).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBMDfrEXVYRx"
   },
   "source": [
    "# 평가 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReCqQ_PIXuor"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    over_threshold = y_true >= 0.1\n",
    "    \n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    remove_NAs = y_true >= 0\n",
    "    \n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
    "\n",
    "def fscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_keras')\n",
    "    return score\n",
    "\n",
    "def maeOverFscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_mse') \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87ba7tiNXxeq"
   },
   "outputs": [],
   "source": [
    "def mae_over_fscore(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: sample_submission.csv 형태의 실제 값\n",
    "    y_pred: sample_submission.csv 형태의 예측 값\n",
    "    '''\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.reshape(1, -1)[0]  \n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    # 실제값이 0.1 이상인 픽셀의 위치 확인\n",
    "    IsGreaterThanEqualTo_PointOne = y_true >= 0.1\n",
    "    \n",
    "    # 실제 값에 결측값이 없는 픽셀의 위치 확인 \n",
    "    IsNotMissing = y_true >= 0\n",
    "    \n",
    "    # mae 계산\n",
    "    mae = np.mean(np.abs(y_true[IsGreaterThanEqualTo_PointOne] - y_pred[IsGreaterThanEqualTo_PointOne]))\n",
    "    \n",
    "    # f1_score 계산 위해, 실제값에 결측값이 없는 픽셀에 대해 1과 0으로 값 변환\n",
    "    y_true = np.where(y_true[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    # f1_score 계산    \n",
    "    f_score = f1_score(y_true, y_pred) \n",
    "    \n",
    "    # f1_score가 0일 나올 경우를 대비하여 소량의 값 (1e-07) 추가 \n",
    "    return mae / (f_score + 1e-07) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cj2Tyt_VYth"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZmKzgwH0kg_"
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr5eyyM5XKxL"
   },
   "outputs": [],
   "source": [
    "def conv2d(inputs,filter_size,kernel_size=3):\n",
    "    \n",
    "    conv = Conv2D(filters=filter_size,kernel_size=3,strides=(1,1),padding='same', activation=None,use_bias=True,kernel_initializer='glorot_normal')(inputs)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = ReLU()(conv)\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def maxpool(conv):\n",
    "    conv = MaxPool2D(pool_size=(2,2),strides=None,padding='valid')(conv)\n",
    "    return conv\n",
    "\n",
    "def deconv2d(conv,filter_size):\n",
    "    conv = Conv2DTranspose(filters=filter_size,kernel_size=(3,3),strides=(2,2),padding='same',dilation_rate=(1,1),activation=None,use_bias=True,kernel_initializer='glorot_normal')(conv)\n",
    "\n",
    "    return conv\n",
    "\n",
    "def concat(deconv,conv):\n",
    "    concat = Concatenate(axis=3)([deconv,conv])\n",
    "\n",
    "    return concat\n",
    "\n",
    "def Contracting_Path(inputs,layers=3,base_feature=32):\n",
    "    dw_conv_list = []\n",
    "    feature = base_feature\n",
    "\n",
    "    # Contraction Path\n",
    "    ## Conv Layer 1\n",
    "    conv = conv2d(inputs,base_feature)\n",
    "    conv = conv2d(conv,base_feature)\n",
    "    dw_conv_list.append(conv)\n",
    "\n",
    "    conv = maxpool(conv)\n",
    "\n",
    "    # Conv Layer 2\n",
    "    for layer in range(layers-2):\n",
    "        feature = feature * 2\n",
    "        conv = conv2d(conv,feature)\n",
    "        conv = conv2d(conv,feature)\n",
    "        dw_conv_list.append(conv)\n",
    "\n",
    "        conv = maxpool(conv)\n",
    "\n",
    "    ## Conv Layer 3\n",
    "    feature = feature * 2\n",
    "    conv = conv2d(conv,feature)\n",
    "    conv = conv2d(conv,feature)\n",
    "\n",
    "    return conv , dw_conv_list\n",
    "\n",
    "def Expanding_Path(conv,dw_conv_list):\n",
    "    \n",
    "    while len(dw_conv_list) != 0:\n",
    "        dw_conv = dw_conv_list.pop()\n",
    "        feature = dw_conv.shape[3]\n",
    "\n",
    "        deconv = deconv2d(conv,feature)\n",
    "        conv_cat = concat(deconv,dw_conv)\n",
    "\n",
    "        conv = conv2d(conv_cat , (conv_cat.shape[3]//2))\n",
    "        conv = conv2d(conv , (conv.shape[3]))\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9NlwqA1XQTe"
   },
   "outputs": [],
   "source": [
    "def Unet():\n",
    "\n",
    "    inputs = Input([40,40,9])\n",
    "\n",
    "    # unet\n",
    "    conv , dw_conv_list = Contracting_Path(inputs,layers=4,base_feature=32)\n",
    "\n",
    "    conv = Expanding_Path(conv,dw_conv_list)\n",
    "\n",
    "    output = Conv2D(filters=1,kernel_size=3,strides=(1,1),padding='same', activation=None,use_bias=True,kernel_initializer='glorot_normal')(conv)\n",
    "    output = SpatialDropout2D(0.25)(output)\n",
    "    output = ThresholdedReLU(theta=0.08)(output)\n",
    "\n",
    "    model = Model(inputs=inputs,outputs=output)\n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[fscore_keras,maeOverFscore_keras])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwsD-74tsOBz"
   },
   "outputs": [],
   "source": [
    "# model = Unet()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtgVjE8b0ex4"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZvyxyu3X2hd"
   },
   "outputs": [],
   "source": [
    "# ReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=3) # factor=0.05\n",
    "# EarlyStop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=7,restore_best_weights=True)\n",
    "\n",
    "# callbackList = [EarlyStop,ReduceLROnPlateau]\n",
    "\n",
    "# hist = model.fit(x_train,y_train,epochs=50,batch_size=512,validation_split=0.25, callbacks = callbackList, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJWl2iZ9b5yC"
   },
   "outputs": [],
   "source": [
    "def train_model(x_data, y_data, k, s):\n",
    "    models = []\n",
    "    k_fold = KFold(n_splits=k, shuffle=True, random_state=777)\n",
    "    \n",
    "    model_number = 0\n",
    "    for train_idx, val_idx in k_fold.split(x_data):\n",
    "        if model_number != s:\n",
    "            x_trainImg, y_trainLabel = x_data[train_idx], y_data[train_idx]\n",
    "            x_valImg, y_valLabel = x_data[val_idx], y_data[val_idx]\n",
    "\n",
    "            model = Unet()\n",
    "\n",
    "            ReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=3) # factor=0.05\n",
    "            EarlyStop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
    "            #ModelCkpt = tf.keras.callbacks.ModelCheckpoint(filepath='ckpt/Unet_' + str(model_number)+'.h5',monitor='val_loss',save_best_only=True)\n",
    "            callbackList = [EarlyStop,ReduceLROnPlateau]\n",
    "            \n",
    "            print('val:',model_number+1)\n",
    "            model.fit(x_trainImg, y_trainLabel, epochs=50, batch_size=512, validation_data=(x_valImg, y_valLabel), callbacks=callbackList)\n",
    "            models.append(model)\n",
    "\n",
    "            del x_trainImg , y_trainLabel , x_valImg , y_valLabel\n",
    "            model_number += 1\n",
    "            \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6273651,
     "status": "ok",
     "timestamp": 1590026036446,
     "user": {
      "displayName": "조준형",
      "photoUrl": "",
      "userId": "07026978245339768740"
     },
     "user_tz": -540
    },
    "id": "Lt6ASeW1d2_2",
    "outputId": "1ec8268d-6b50-4f43-bb97-92fa091f482b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 1\n",
      "Train on 57728 samples, validate on 14433 samples\n",
      "Epoch 1/50\n",
      "57728/57728 [==============================] - 63s 1ms/sample - loss: 0.1275 - fscore_keras: 0.4281 - maeOverFscore_keras: 4.5307 - val_loss: 0.1438 - val_fscore_keras: 0.1310 - val_maeOverFscore_keras: 16.4849\n",
      "Epoch 2/50\n",
      "57728/57728 [==============================] - 61s 1ms/sample - loss: 0.1164 - fscore_keras: 0.5320 - maeOverFscore_keras: 3.2168 - val_loss: 0.1423 - val_fscore_keras: 0.1715 - val_maeOverFscore_keras: 12.2586\n",
      "Epoch 3/50\n",
      "57728/57728 [==============================] - 61s 1ms/sample - loss: 0.1127 - fscore_keras: 0.5661 - maeOverFscore_keras: 2.9154 - val_loss: 0.1428 - val_fscore_keras: 0.4217 - val_maeOverFscore_keras: 3.5753\n",
      "Epoch 4/50\n",
      "57728/57728 [==============================] - 61s 1ms/sample - loss: 0.1112 - fscore_keras: 0.5781 - maeOverFscore_keras: 2.8197 - val_loss: 0.1295 - val_fscore_keras: 0.4801 - val_maeOverFscore_keras: 3.5914\n",
      "Epoch 5/50\n",
      "57728/57728 [==============================] - 61s 1ms/sample - loss: 0.1108 - fscore_keras: 0.5828 - maeOverFscore_keras: 2.7813 - val_loss: 0.1365 - val_fscore_keras: 0.3917 - val_maeOverFscore_keras: 5.1136\n",
      "Epoch 6/50\n",
      "57728/57728 [==============================] - 62s 1ms/sample - loss: 0.1097 - fscore_keras: 0.5923 - maeOverFscore_keras: 2.7071 - val_loss: 0.1367 - val_fscore_keras: 0.4674 - val_maeOverFscore_keras: 3.6153\n",
      "Epoch 7/50\n",
      "57728/57728 [==============================] - 61s 1ms/sample - loss: 0.1091 - fscore_keras: 0.5963 - maeOverFscore_keras: 2.6745 - val_loss: 0.1249 - val_fscore_keras: 0.4549 - val_maeOverFscore_keras: 3.4327\n",
      "Epoch 8/50\n",
      "57728/57728 [==============================] - 61s 1ms/sample - loss: 0.1083 - fscore_keras: 0.6033 - maeOverFscore_keras: 2.6240 - val_loss: 0.1104 - val_fscore_keras: 0.6053 - val_maeOverFscore_keras: 2.6365\n",
      "Epoch 9/50\n",
      "57728/57728 [==============================] - 62s 1ms/sample - loss: 0.1079 - fscore_keras: 0.6066 - maeOverFscore_keras: 2.5994 - val_loss: 0.1165 - val_fscore_keras: 0.5705 - val_maeOverFscore_keras: 2.7514\n",
      "Epoch 10/50\n",
      "33280/57728 [================>.............] - ETA: 21s - loss: 0.1089 - fscore_keras: 0.6159 - maeOverFscore_keras: 2.5691"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "k = 5\n",
    "preds = []\n",
    "\n",
    "models = train_model(x_train, y_train, k=k, s=4)\n",
    "\n",
    "for model in models:\n",
    "    model.save('ckpt/Unet_val_'+str(n)+'.h5')\n",
    "    n += 1\n",
    "\n",
    "for model in models:\n",
    "    preds.append(model.predict(x_test))\n",
    "    print(mae_over_fscore(y_test, preds[-1]))\n",
    "\n",
    "preds = np.array(preds)\n",
    "pred = np.array(preds).mean(axis=0)\n",
    "print(mae_over_fscore(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOM5YZYitM3v"
   },
   "outputs": [],
   "source": [
    "# model.evaluate(x_test,y_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SF4ux51fJPS_"
   },
   "outputs": [],
   "source": [
    "# val test\n",
    "preds = []\n",
    "for model in models:\n",
    "    preds.append(model.predict(x_test))\n",
    "    print(mae_over_fscore(y_test, preds[-1]))\n",
    "\n",
    "preds = np.array(preds)\n",
    "predictions = preds.mean(axis=0)\n",
    "print(mae_over_fscore(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DA-Tpia68nRZ"
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "preds = []\n",
    "for model in models:\n",
    "    preds.append(model.predict(test_image))\n",
    "\n",
    "preds = np.array(preds)\n",
    "prediction = np.array(preds).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0r4FrViwuu06"
   },
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('data/sample_submission.csv')\n",
    "# submission.iloc[:,1:] = prediction.reshape([-1,1600])\n",
    "# submission.to_csv('data/U_Net_ver1_2_20200521.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lgP6c3nQzqU"
   },
   "outputs": [],
   "source": [
    "# model.save('U_Net_200521_ver_1_2.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "U-Net ver.2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
